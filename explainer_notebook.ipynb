{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 1. Motivation\n",
    "## 1.1 Dataset\n",
    "The main dataset was the NYC Street Tree Data from 2015 and is the result of a community survery done mainly by volunteers, cataloging all trees in NYC. As secondary data sets we had the ones regarding Street Tree Data from 1995 and 2005. Moreover, we used the air pollution data in New York City, in order to understand the influence of trees on the air quality. We also started analyzing the \"311\" dataset, to explore some complaints regarding trees.\n",
    "The Street Tree dataset was chosen because it could give new insights and perspective to urban planning, discover their status (how healthy they are, if people are taking care of those etc.), and if they are influencing the life quality of the city. Moreover we could have discovered facts that most people would probably not be aware of beforehand.  \n",
    "\n",
    "\n",
    "## 1.2 Goal\n",
    "The goal was to enlighten users about trees in NYC. Are there certain types of trees more suitable for streets than others? Where are they located? Is it possible to know which kind of tree you might encounter based on the location, health of the tree, the diameter, or even the amount of problems of the tree? From this project it should be possible to learn something new about a topic you might never have considered learning something about."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Basic stats\n",
    "## 2.1 Preprocessing the data\n",
    "\n",
    "There were some outlies in the dataset which had to be removed to get useful results. One example was lat/lon which had an extreme outlier. \n",
    "\n",
    "Regarding the 311 dataset, we just selected the 2015 data and the complaints regarding trees, as these were the only ones important for our domain.\n",
    "\n",
    "For the air pollution there was data for all the community districts, but only some of the neighbourhoods. The measurements were mean percentile. We took the mean values for the community district and and assigned them to the corresponding borough. This was because the names of the neighbourhoods in this dataset and our own dataset were so different that it was very difficult to figure out which neighbourhoods were the same in the two sets.\n",
    "\n",
    "### 2.1.1 Variable selection\n",
    "When taking a first glance at the dataset it was a bit overwhelming, as it's huge and there are a lot of rows which does not necessarily make sense at a first glance, as wel as a number of of variables not particularly interesting or necessary for what we wanted to do. Each variable was carefully examined and the variables deemed unnecessary were excluded. Among these was \"Tree_Id\", a unique ID for each tree, but this unique ID was unique for each of the three datasets (1995, 2005, 2015), meaning it was not possible to join the datasets by this ID, deeming it not relevant. Other excluded variables were address information, since there were multiple variables delivering address information on different levels - and it was not relevant to distinguish between all these. \n",
    "\n",
    "### 2.1.2 Observation selection\n",
    "It was decided to only focus on the top 20 tree species, since there were a lot of different species without a significant amount of observations, it would be difficult to describe them all properly. It would also be very difficult to do good predictions if the observations are sparse. \n",
    "For some machine learning tools, we focused only on the top 10 species, or top 5, because the data was too sparse when going above this limit.\n",
    "\n",
    "There were a lot of trees without a species listed, and those were disregarded completely. The dead trees were also excluded from the dataset. \n",
    "\n",
    "It was considered to only focus on one of the five boroughs in NYC to get a more detailed view. This was not implemented since it was deemed more interesting to two differences between the boroughs as well. \n",
    "\n",
    "\n",
    "## 2.2 Stats for the preprocessed data\n",
    "The final dataset \"Street Tree Data 2015\" consists of 534,514 tree observations and 21 variables/features, totalling 74.5 MB.\n",
    "The selected features were: \n",
    "* Diameter (inches)\n",
    "* Health (three values: Good, fair, poor)\n",
    "* Spc_Latin, Spc_Common (latin and common name for the species) \n",
    "* Sidewalk_Condition (two values: Damage, NoDamage)\n",
    "* Problems (a string concatenated from the following types of problems)\n",
    " * root_stone, root_grate, root_other, trunk_wire, trnk_light, trnk_other, brch_light, brch_shoe, brch_other (two values: yes/no)\n",
    "* Address\n",
    "* Zipcode\n",
    "* CB (community board)\n",
    "* Borough\n",
    "* Latitude, Longitude\n",
    "\n",
    "Amount of trees in each borough:\n",
    "- Bronx: 63,035\n",
    "- Brooklyn: 138,760\n",
    "- Staten Island: 82,619\n",
    "- Manhattan: 54,115\n",
    "- Queens: 195,985\n",
    "\n",
    "In general, the top 20 species were the same for the 5 boroughs, but the order of this \"top 20\" list was different. There were more trees with general problems in Manhattan as well as more unhealthy trees. \n",
    "\n",
    "We did a lot of Pearson correlation among the different variables for figuring out that not a lot of the variables were correlated. In the end, finally figured out some correlation between the air quality, tree amount and tree diameter and the health states.\n",
    "\n",
    "But let's start looking at the main dataset, the 2015 Street Tree Census (https://data.cityofnewyork.us/Environment/2015-Street-Tree-Census-Tree-Data/uvpi-gqnh).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 2.3 Other datasets inspected\n",
    "Multiple secondary datasets were inspected, e.g. the 311 dataset and the airpollution dataset, as well as the Street Tree datasets from 2005 and 1995, respectively. In the 311 set, there were several complaints about trees in NYC. No significant correlations were found though. It was hoped that a connected between a certain type of complaint were correlated with different problems or the health of the tree, but unfortunately data does not always behave as hoped or suspected, and patterns cannot (and should not) be forced to appear. \n",
    "\n",
    "One could also be inclined to wonder if more \"green\" areas, meaning areas with a lot of trees, had higher house prices. Again, after investigation, this was found to be challenging, since there is not a lot of information about house prices available - at least not on a neighbourhood level. \n",
    "\n",
    "It was also considered if there was a correlation between the trees/features of the trees and the air pollution. This dataset was used for simple linear regression.\n",
    "\n",
    "For the different maps used, a few other datasets have been included in the shape of geojson files, which includes the data needed for drawing the d3 maps (polygons) as well as basic information about the parts of the city, they're representing, such as borough, community district etc., which was used in combination with our own data from the Street Tree dataset to produce interactive maps. The geojsons used can be found and downloaded at (https://github.com/cecli/cecli.github.io/tree/master/data/geojson).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Theory\n",
    "\n",
    "## 3.1 Machine Learning tools\n",
    "When doing predictions it can be difficult to find the appropriate tools to use. Different tools have different qualities and it all depends on the data and the patterns in your data. In this project, different tools have been tried out, typically multiple tools for the same prediction to inspect the model performance of each tool. \n",
    "\n",
    "### 3.1.1 KNN\n",
    "KNN is a tool rather easy to grasp and implement. It was chosen for predicting the health of a tree based on GPS coordinates, as well as predicting species based on GPS coordinates. An argument for KNN being the most appropriate choice is that one could think that when planting trees, one would be inclined to plant the same trees together. One could also think that unhealthy trees are likely in the same area, presumable because of a decease in the area, a pollution problem, soil problems or something completely different. A drawback of the KNN method is that when dealing when an unbalanced dataset it will favour the most occuring observation. \n",
    "\n",
    "### 3.1.2 Decision trees and Random Forest\n",
    "Decision trees can often be a good choice because they are nice to visualize. A drawback is that they tend to overfit the training data. It was used for predicting health based on GPS coordinates, as well as species based on GPS coordinates in spite of its drawback. And also, we use it for predicting the tree species based on location and predicting diameter based on species and location. When predicting species different features were added to see if they contributed to the predictions, e.g. the diameter. The main reason was to compare with the other results. If the decision trees did not overfit and still performed well, then it would be nice to visualize. To accomodate the overfitting issue a random forest was also tried out.\n",
    "\n",
    "Decision trees were also used to predict diameter based on species and problems, as well as predicting diamater based on the amount of problems. Here, the diameter was binned in bins of different sizes (1-10, 10-15, 15-20, ..., 45-50, 50-60, 60-70, ..., 90-100, 100-150, 150-200, ...)\n",
    "\n",
    "### 3.1.3 SVM\n",
    "As a third tool, Support Vector Machines were tried out. SVMs can do linear classification by creating a \"maximum seperating hyperplane\" between data. It can also do non-linear classification using a so-called kernel-trick where inputs are mapped to high-dimensional feature space. This was used to predict health based on GPS coordinates. \n",
    "\n",
    "### 3.1.4 Apriori\n",
    "Apriori is an algorithm for frequent item search. It was used to inspect problems appearing together. We used this algorithm for seeing if some problems were appearing together in the same observation. We found that some problems are sometimes appearing in the same trees.\n",
    "\n",
    "### 3.1.5 Linear and multiple regression\n",
    "Linear regression was used to inspect correlation between different features, and is not really a machine learning tool as much as a tool for investigating linear correlations. It was used to predict air pollution based on the amount of trees as well as diameter. \n",
    "\n",
    "## 3.2 Model selection\n",
    "When selecting appropriate models, first thing is to split the data into a training set and a test set. \n",
    "When predicting health (or species) based on GPS coordinates, a test set consisting of 15% of the total amount of observations was used. Hereafter the training set was \"split\" into a training set and a validation set, using a 5-fold and 10-fold cross-validation. The best model was chosen based on accuracy scored, but with computation time taken into account as well. For the KNN, different values of $k$ was tried, ranging from $K=2,...,10$. The limit was set to 10 because it was not expected that we would have a whole area of unhealthy trees, and that might just confuse the predictions. \n",
    "\n",
    "## 3.3 Model performance\n",
    "For predicting species and health based on GPS coordinates, KNN was selected as the best model. SVM simply took a significant amount of time to run, making it difficult to fine tune and handle. Decision trees overfitted the training data and was not good at handling sparse data.\n",
    "\n",
    "For predicting species, the KNN classifier was able to predict $51.7$% accurately for $K=4$ on the test data, whereas the average (average over the 5 folds) performance on the validation set were $49.9$% (compared to $49$% for decision trees). This is considered rather good, taken into account that it is labelling $20$ different species, but it would also suggest that the same species is not always planted next to each other. They are actually not planted next to each other more often than expected before investigating the data. \n",
    "    In comparison, when only trying to predict the top 5 species instead of all 20 species, the average performance on the validation set were $70.4$% for $K=4$. This also confirms that the same species are not always planted next to each other, and shows, as expected, that the model performs better when addressing fewer species. \n",
    "\n",
    "For predicting health, the KNN classifier was able to predict $80.7$% accurately for $K=5$ on the test data, whereas the average (over the 5 folds) performance on the valdiation set were $80.4$% (compared to $74.8$% for decision trees). An accuracy of $80.7$% is rather good considering the sparsity of the \"fair\" and \"poor\" tree observations. The KNN did handle the sparsity trees better than the decision tree classifier. When it labeled a tree as \"fair\" it had around $43$% correct (on the validation set for $K=5$). A bit worse it went for the \"poor\" classifications, here it only predicted around $1/3$ correctly. It was, as we thought, much better at predicting the good trees. This makes sense since there were a lot more training data available. In comparison, the decision tree classifier had around $31$% correct for the \"fair\" trees, and $17$% for the \"poor\" trees. The amount of misclassifications on the \"poor\" and \"fair\" trees suggests that the condition of the trees do not really reflect on its neighbours and are most likely caused by other, individual things.\n",
    "\n",
    "The decision trees were performing almost like the random forest in our case. When using species as an input, we obtained a maximum accuracy of 0.5 with the top 5 species. With top 10 and 20, the accuracy was even less, which made us conclude it was not a good model. Moreover, we saw that in the visualization using graphviz, it was predicting just two species, Locust and Pine Oak, so it was definitely not a good tool for our case.\n",
    "\n",
    "The apriori algorithm found some correlation among the stone problems. The score was not that good, but we saw it was improving when taking into account just the trees with some problems or the trees with just one problem. This is understandable, because some of the trees might be new, which can also be seen on the diameter of these trees. So it is possible that they don't have a lot of problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Visualizations\n",
    "\n",
    "## 4.1 Borough-basics\n",
    "The d3 pie charts show the basic distributions of the data: Amount of trees in each borough and the size of each borough. They were chosen so the user was able to get a clear view of the differences between the boroughs. As comparison, the other pie chart shows the size each borough. Here, it is obvious that the amount of trees in each borough roughly corresponds to the size of the borough, and the exact percentage can be viewed when hovering over each borough. \n",
    "\n",
    "These charts are important for the project since they introduce the user to the topic: What are the different boroughs, how are they different with regards to size and amount of trees? \n",
    "\n",
    "## 4.2 Tree distribution of top 5 species\n",
    "The d3 bar chart show the distribution of the 5 most common species in NYC for each borough, as well as NYC. This illustrates the borough-wise differences regarding the most common trees. E.g. the London Planetree is the most common tree in NYC. It is also the most common tree in Queens. Queens is the largest area in NYC both regarding trees and size, as shown in the pie charts. Therefore it might be that most of NYC's London Planetrees are located in Queens, but as it can be seen from the chart, most of them actually comes from Brooklyn. The chart also have the option of changing the years from 2015 to either 2005 or 1995, and hereby see the differences over time as well. \n",
    "\n",
    "This chart is important for the project since it enables the user to view the distribution of the most common trees in NYC on borough level for the three different years available: 1995, 2005, 2015. This helps us show the changes in street trees in NYC over time.\n",
    "\n",
    "## 4.3 Fun facts about the top 10 species\n",
    "This page enables the user to hover over pictures of leaves for the 10 most common species to see the species name, and then when clicking a leaf image, fun facts about these trees appear. \n",
    "\n",
    "This function is important for the project since it sets each of the trees in a context: What is it called? What is its ranking? How many trees are there of this species? What is special/interesting about the species? Why is it a good street tree compared to others? \n",
    "\n",
    "## 4.4 Health map of the NYC street trees\n",
    "The d3 map enables the user to view a prediction of the health of the NYC street trees, using KNN as a classifier. It is possible to hover over the individual boroughs for details, and to switch between visualizing the good, fair, and poor trees. When hovering over a borough, a tooltip is displayed, showing information in the shape of borough name, borough size, amount of trees in that borough specified by the tree species, and finally, the percentages of 'good' and 'poor' trees in that borough.\n",
    "\n",
    "The map is an important visualization for the project since it shows the location of the trees in regards to different health. Because of the large amount of data points it would have created a too confusing picture to visualize all three health states at the same time, which is why it has been split up into 'good' and 'poor' with the 'fair' trees available to append at the click of a button.\n",
    "\n",
    "## 4.5 Scatterplot\n",
    "The site includes three scatterplots. The first scatterplot visualizes the most common problems trees can have according to the dataset. The problems visualized are split into three different ones: 'Trunk', 'Root' and 'Branch'. These problems are caused by humans, such as trees growing into phone lines, wires around the trunk or stones on the root. Further definitions can be found in the dataset manual in the link provided. \n",
    "\n",
    "The second and the third scatterplot shows the correlation among the amount of trees, diameter and air pollution in each of the neighbourhoods. We just took the community district points and assign the mean of each neighbourhood. The user can explore each neighbourhood when hovering over the map and the corresponding plots in the scatterplot. \n",
    "\n",
    " * http://bl.ocks.org/weiglemc/6185069\n",
    " * https://bl.ocks.org/ctufts/674ece47de093f6e0cd5af22d7ee9b9b\n",
    " * http://bl.ocks.org/weiglemc/6185069"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Discussion\n",
    "When working with the project, two things became clear to us: \n",
    "1) Real life data is messy\n",
    "2) Data do not care what you think of them\n",
    "\n",
    "It is possible to have good intentions, and a lot of good ideas as to what to do when analyzing a dataset, but the data itself just have limitations that are not always possible to overcome. \n",
    "\n",
    "## 5.1 What went well?\n",
    "During the project, a lot of things did not go as expected. First of all, the pattern we expected to find in the data was just not there. The intention was to find correlation between the problems of the trees and the health, possible also correlation with the diameter. A lot of basic Pearson correlations was done on the data, but it appeared that there actually were no significant correlations. Then a lot of other datasets were inspected to see if they were correlated with some of the tree data features. Not a lot of great things appeared, so we tried keeping to the basics and that went quite well.\n",
    "\n",
    "We managed to create a lot of plots and visualizations of the data, showing the fundamentals counts. We also managed to apply different machine learning methods, though they only showed us what the preliminary analysis did: In general, there were not really large areas with problematic trees, and that could not really be related to the health. The health was sparse, influencing the predictions. \n",
    "\n",
    "## 5.2 Possible improvements\n",
    "If there was more data available to join with the street tree data, we might have been able to find a nice pattern/correlation, but in spite of our efforts to do so, we did not manage to find such data set. \n",
    "\n",
    "We could also have focused on one prediction goal instead of trying to find a lot of different patterns, that turned out to not be there. E.g. we could have focused on predicting species based but with some other tools, since KNN obviously was not the best choice. A suggestion would be to try do some binary classifications, locating e.g. London Planetrees using SVMs.\n",
    "\n",
    "## 5.3 What is still missing?\n",
    "In the end, what we actually missed was patterns in the data so we could have done some more advanced and great predictions. But patterns cannot be forced, so with that in mind, we could have used more visualizations for the predictions we did. We kept the website basic to try to create some beautiful visualizations of the basic stats instead of trying to visualize predictions in different ways. This choice was made based on the lack of good patterns. We did not really want to show a lot of predictions if they were not actually useful. \n",
    "\n",
    "We could have assigned the air pollution to each neighbourhood available in the dataset instead of the community district mean. This was too much work because of the different neighbourhood names. Moreover, we could have explore the 311 dataset more than we did to see if some areas with more complaints performed worse in regards to air pollution.\n",
    "\n",
    "We also started analyzing house prices, but the neighbourhood names were too different, so time prohibited us from continuing down this path.\n",
    "\n",
    "The visualization of the map on the site is a bit slow, and we could not figure out how to optimize it. Moreover, the scatterplots were not working initially so we spent a lot of time figuring out how to improve the site in regards to these and one axis is even still missing in the second scatterplot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix: Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Import data the whole dataset \n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "tree_data = pd.read_csv('2015_tree_data_updated.csv')\n",
    "tree_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Convert health categories to numbers. 1: Poor, 2: Fair, 3: Good. Higher = Better\n",
    "health = []\n",
    "for i in range(len(tree_data['Health'])):\n",
    "    if tree_data['Health'][i] == 'Good':\n",
    "        health.append(3)\n",
    "    elif tree_data['Health'][i] == 'Fair':\n",
    "        health.append(2)\n",
    "    elif tree_data['Health'][i] == 'Poor':\n",
    "        health.append(1)\n",
    "    else:\n",
    "        health.append(0)\n",
    "        #print \"err\", tree_data['Health'][i], i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Finding the total number of trees and how many there are of the different species of trees\n",
    "tree_amount = tree_data['Spc_Common'].value_counts()\n",
    "\n",
    "print('Trees: %d' % tree_data15.shape[0])\n",
    "print(tree_amount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Plotting the results to get an overview\n",
    "plt.style.use('ggplot')\n",
    "%matplotlib inline\n",
    "\n",
    "def barplot(series, title, figsize, ylabel, flag, rotation):\n",
    "    ax = series.plot(kind='bar', \n",
    "                title = title,\n",
    "                figsize = figsize,\n",
    "                fontsize = 13)\n",
    "    \n",
    "    # set ylabel\n",
    "    ax.set_ylabel(ylabel)\n",
    "    # set xlabel (depending on the flag that comes as a function parameter)\n",
    "    ax.get_xaxis().set_visible(flag)\n",
    "    # set series index as xlabels and rotate them\n",
    "    ax.set_xticklabels(series.index, rotation= rotation)\n",
    "    \n",
    "barplot(tree_amount,'Tree types', figsize=(20,8), ylabel = 'tree count',flag = True, rotation = 90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Putting the percentages on a pie chart\n",
    "ax = tree_amount15.plot(kind='pie', title='Top 20 tree species in NYC', autopct='%1.0f%%', pctdistance=0.9)\n",
    "ax.set_ylabel('')\n",
    "ax.set_aspect('equal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Count no. of trees in each borough:\n",
    "boros = tree_data['Borough'].unique()\n",
    "print tree_data['Borough'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Finding how many trees there are of the different health types\n",
    "tree_health = tree_data['Health'].value_counts()\n",
    "print(tree_health)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Comparing the count of each tree species in the whole city with a borough. This was used in the initial analysis to try and \n",
    "#determine if focus should be put on a single borough, and which borough this should be.\n",
    "queens_tree_types = tree_data.loc[tree_data['Borough'] == 'Queens', 'Spc_Common'].value_counts()\n",
    "brooklyn_tree_types = tree_data.loc[tree_data['Borough'] == 'Brooklyn', 'Spc_Common'].value_counts()\n",
    "staten_tree_types = tree_data.loc[tree_data['Borough'] == 'Staten Island', 'Spc_Common'].value_counts()\n",
    "bronx_tree_types = tree_data.loc[tree_data['Borough'] == 'Bronx', 'Spc_Common'].value_counts()\n",
    "manhattan_tree_types = tree_data.loc[tree_data['Borough'] == 'Manhattan', 'Spc_Common'].value_counts()\n",
    "\n",
    "df = pd.concat([tree_amount, queens_tree_types], axis=1)\n",
    "print(df)\n",
    "df.columns = ['NYC', 'Queens']\n",
    "\n",
    "df = df.sort_values('NYC', ascending=False) # sort the df using NYC values\n",
    "\n",
    "df.plot.bar(color=['red','blue'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Comparing the number of trees in each of the five boroughs\n",
    "fig, axes = plt.subplots(nrows=5)\n",
    "plt.subplots_adjust(wspace=1, hspace=0.5)\n",
    "\n",
    "plot = queens_tree_types.plot(ax=axes[0], kind='bar', figsize=(8,30)); axes[0].set_title('Queens');\n",
    "brooklyn_tree_types.plot(ax=axes[1], kind='bar'); axes[1].set_title('Brooklyn');\n",
    "manhattan_tree_types.plot(ax=axes[2], kind='bar'); axes[2].set_title('Manhattan');\n",
    "staten_tree_types.plot(ax=axes[3], kind='bar'); axes[3].set_title('Staten Island');\n",
    "bronx_tree_types.plot(ax=axes[4], kind='bar'); axes[4].set_title('Bronx');\n",
    "\n",
    "fig = plot.get_figure()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Heatmap of tree distribution\n",
    "X_new=[]\n",
    "Y_new=[]\n",
    "\n",
    "for i in range(len(CB)):\n",
    "    X_new.append(Longitude[i])\n",
    "    Y_new.append(Latitude[i])\n",
    "    \n",
    "with open('Coordinates_trees.csv', 'wb') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(('lon', 'lat'))\n",
    "    for xd, yd in zip(X_new, Y_new):\n",
    "        writer.writerow( (xd, yd ) )\n",
    "    csvfile.close()\n",
    "\n",
    "import geoplotlib\n",
    "from geoplotlib.utils import read_csv, BoundingBox, DataAccessObject\n",
    "\n",
    "min_lat = min(X_new)\n",
    "max_lat = max(X_new)\n",
    "min_lon = min(Y_new)\n",
    "max_lon = max(Y_new)\n",
    "\n",
    "bbox = BoundingBox(north=float(max_lon), west=float(max_lat), south=float(min_lon), east=float(min_lat))\n",
    "print \"Trees:\", bbox\n",
    "\n",
    "data_trees = read_csv('Coordinates_trees.csv')\n",
    "geoplotlib.kde(data_trees, bw=0.5, cmap = 'jet', cut_below=1e-4)\n",
    "geoplotlib.set_bbox(bbox)\n",
    "geoplotlib.inline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the repository, there's also data from 2005 and 1995 but it is not included in this explainer notebook as it does not provide any value for explaining our analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data for histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#2015 data \n",
    "\n",
    "import numpy as np\n",
    "\n",
    "#NYC top 20 species\n",
    "unique, counts = np.unique(zip(tree_data['Spc_Common']), return_counts=True)\n",
    "sorted(zip(counts, unique), reverse = True)\n",
    "\n",
    "#Extract data for each borough\n",
    "tree_data_bronx = tree_data.loc[tree_data['Borough'] == 'Bronx']\n",
    "tree_data_brook = tree_data.loc[tree_data['Borough'] == 'Brooklyn']\n",
    "tree_data_stat = tree_data.loc[tree_data['Borough'] == 'Staten Island']\n",
    "tree_data_manh = tree_data.loc[tree_data['Borough'] == 'Manhattan']\n",
    "tree_data_queens = tree_data.loc[tree_data['Borough'] == 'Queens']\n",
    "\n",
    "#Bronx\n",
    "unique, counts = np.unique(zip(tree_data_bronx['Spc_Common']), return_counts=True)\n",
    "sorted(zip(counts, unique), reverse = True)\n",
    "\n",
    "#Brooklyn\n",
    "unique, counts = np.unique(zip(tree_data_brook['Spc_Common']), return_counts=True)\n",
    "sorted(zip(counts, unique), reverse = True)\n",
    "\n",
    "#Staten Island\n",
    "unique, counts = np.unique(zip(tree_data_stat['Spc_Common']), return_counts=True)\n",
    "sorted(zip(counts, unique), reverse = True)\n",
    "\n",
    "#Manhattan\n",
    "unique, counts = np.unique(zip(tree_data_manh['Spc_Common']), return_counts=True)\n",
    "sorted(zip(counts, unique), reverse = True)\n",
    "\n",
    "#Queens\n",
    "unique, counts = np.unique(zip(tree_data_queens['Spc_Common']), return_counts=True)\n",
    "print \"Queens data:\"\n",
    "print sorted(zip(counts, unique), reverse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#2005 data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "tree_data = pd.read_csv('2005_tree_data_updated.csv')\n",
    "\n",
    "#NYC top 20 species\n",
    "unique, counts = np.unique(zip(tree_data['Spc_Common']), return_counts=True)\n",
    "sorted(zip(counts, unique), reverse = True)\n",
    "\n",
    "#Extract data for each borough\n",
    "tree_data_bronx = tree_data.loc[tree_data['Borough'] == 'Bronx']\n",
    "tree_data_brook = tree_data.loc[tree_data['Borough'] == 'Brooklyn']\n",
    "tree_data_stat = tree_data.loc[tree_data['Borough'] == 5]\n",
    "tree_data_manh = tree_data.loc[tree_data['Borough'] == 'Manhattan']\n",
    "tree_data_queens = tree_data.loc[tree_data['Borough'] == 'Queens']\n",
    "\n",
    "#Bronx\n",
    "unique, counts = np.unique(zip(tree_data_bronx['Spc_Common']), return_counts=True)\n",
    "sorted(zip(counts, unique), reverse = True)\n",
    "\n",
    "#Brooklyn\n",
    "unique, counts = np.unique(zip(tree_data_brook['Spc_Common']), return_counts=True)\n",
    "sorted(zip(counts, unique), reverse = True)\n",
    "\n",
    "#Staten Island\n",
    "unique, counts = np.unique(zip(tree_data_stat['Spc_Common']), return_counts=True)\n",
    "sorted(zip(counts, unique), reverse = True)\n",
    "\n",
    "#Manhattan\n",
    "unique, counts = np.unique(zip(tree_data_manh['Spc_Common']), return_counts=True)\n",
    "sorted(zip(counts, unique), reverse = True)\n",
    "\n",
    "#Queens\n",
    "unique, counts = np.unique(zip(tree_data_queens['Spc_Common']), return_counts=True)\n",
    "print \"Queens data:\"\n",
    "print sorted(zip(counts, unique), reverse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#1995 data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "tree_data = pd.read_csv('1995_tree_data_updated.csv')\n",
    "\n",
    "#NYC top 20 species\n",
    "unique, counts = np.unique(zip(tree_data['Spc_Common']), return_counts=True)\n",
    "sorted(zip(counts, unique), reverse = True)\n",
    "\n",
    "#Extract data for each borough\n",
    "tree_data_bronx = tree_data.loc[tree_data['Borough'] == 'Bronx']\n",
    "tree_data_brook = tree_data.loc[tree_data['Borough'] == 'Brooklyn']\n",
    "tree_data_stat = tree_data.loc[tree_data['Borough'] == 'Staten Island']\n",
    "tree_data_manh = tree_data.loc[tree_data['Borough'] == 'Manhattan']\n",
    "tree_data_queens = tree_data.loc[tree_data['Borough'] == 'Queens']\n",
    "\n",
    "#Bronx\n",
    "unique, counts = np.unique(zip(tree_data_bronx['Spc_Common']), return_counts=True)\n",
    "sorted(zip(counts, unique), reverse = True)\n",
    "\n",
    "#Brooklyn\n",
    "unique, counts = np.unique(zip(tree_data_brook['Spc_Common']), return_counts=True)\n",
    "sorted(zip(counts, unique), reverse = True)\n",
    "\n",
    "#Staten Island\n",
    "unique, counts = np.unique(zip(tree_data_stat['Spc_Common']), return_counts=True)\n",
    "sorted(zip(counts, unique), reverse = True)\n",
    "\n",
    "#Manhattan\n",
    "unique, counts = np.unique(zip(tree_data_manh['Spc_Common']), return_counts=True)\n",
    "sorted(zip(counts, unique), reverse = True)\n",
    "\n",
    "#Queens\n",
    "unique, counts = np.unique(zip(tree_data_queens['Spc_Common']), return_counts=True)\n",
    "print \"Queens data:\"\n",
    "print sorted(zip(counts, unique), reverse = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problems exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Loading the data\n",
    "import csv\n",
    "\n",
    "# open the file in universal line ending mode \n",
    "#the file called 2015_tree_data_updated.csv is exactly the same download for the first assignment, jusr re-named.\n",
    "#This file is not included in the repo because is very big\n",
    "with open('2015_tree_data_updated.csv', 'r') as infile:\n",
    "    # read the file as a dictionary for each row ({header : value})\n",
    "    reader = csv.DictReader(infile)\n",
    "    data = {} #empty set\n",
    "    for row in reader:\n",
    "        for header, value in row.items():\n",
    "            try:\n",
    "                data[header].append(value)\n",
    "            except KeyError:\n",
    "                data[header] = [value]\n",
    "Diameter = data['Diameter']\n",
    "Health = data['Health']\n",
    "Spc_Latin = data['Spc_Latin']\n",
    "Spc_Common = data['Spc_Common']\n",
    "Sidewalk_Condition = data['Sidewalk_Condition']\n",
    "problems = data['problems']\n",
    "root_stone = data['root_stone']\n",
    "root_grate = data['root_grate']\n",
    "root_other = data['root_other']\n",
    "trunk_wire = data['trunk_wire']\n",
    "trnk_light = data['trnk_light']\n",
    "trnk_other = data['trnk_other']\n",
    "brch_light = data['brch_light']\n",
    "brch_shoe = data['brch_shoe']\n",
    "brch_other = data['brch_other']\n",
    "Address = data['Address']\n",
    "Zipcode = data['Zipcode']\n",
    "CB = data['CB']\n",
    "Borough = data['Borough']\n",
    "Latitude = data['Latitude']\n",
    "Longitude = data['Longitude']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bor_list = list(set(Borough))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bronx_prob_list = [0, 0, 0, 0, 0]\n",
    "brooklyn_prob_list = [0, 0, 0, 0, 0]\n",
    "staten_prob_list = [0, 0, 0, 0, 0]\n",
    "man_prob_list = [0, 0, 0, 0, 0]\n",
    "queens_prob_list = [0, 0, 0, 0, 0]\n",
    "\n",
    "dic_all_boro = {}\n",
    "for b in bor_list:\n",
    "    dic_all_boro[b] = [0, 0, 0, 0, 0]\n",
    "\n",
    "temp_root = 0\n",
    "temp_trunk = 0\n",
    "temp_branch = 0\n",
    "temp_tot = 0\n",
    "sidewalk = 0\n",
    "for i in range (0, len(CB)):\n",
    "    if root_stone[i] == 'Yes':\n",
    "        temp_root += 1\n",
    "    if root_grate[i] == 'Yes':\n",
    "        temp_root += 1\n",
    "    if root_other[i] == 'Yes':\n",
    "        temp_root += 1\n",
    "    if trunk_wire[i] == 'Yes':\n",
    "        temp_trunk += 1\n",
    "    if trnk_light[i] == 'Yes':\n",
    "        temp_trunk += 1\n",
    "    if trnk_other[i] == 'Yes':\n",
    "        temp_trunk += 1\n",
    "    if brch_light[i] == 'Yes':\n",
    "        temp_branch += 1\n",
    "    if brch_shoe[i] == 'Yes':\n",
    "        temp_branch += 1\n",
    "    if brch_other[i] == 'Yes':\n",
    "        temp_branch += 1\n",
    "    if Sidewalk_Condition[i] == 'Damage':\n",
    "        sidewalk += 1\n",
    "    temp_tot = temp_root + temp_trunk + temp_branch + sidewalk\n",
    "    temp_list = [temp_root, temp_trunk, temp_branch, sidewalk, temp_tot]\n",
    "    \n",
    "    #choose which list to update\n",
    "    c = 0\n",
    "    for t in temp_list:\n",
    "        dic_all_boro[Borough[i]][c] += t\n",
    "        c += 1\n",
    "    temp_root = 0\n",
    "    temp_trunk = 0\n",
    "    temp_branch = 0\n",
    "    temp_tot = 0\n",
    "    sidewalk = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('problem_count.csv', 'wb') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(('Borough','Root_Prob', 'Trunk_Prob', 'Branch_Prob', 'Sidewalk', 'Tot_Prob'))\n",
    "    for d in dic_all_boro.keys():\n",
    "        writer.writerow((d, dic_all_boro[d][0], dic_all_boro[d][1], dic_all_boro[d][2], dic_all_boro[d][3], dic_all_boro[d][4] ))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bronx_prob_list = [0, 0, 0, 0, 0]\n",
    "brooklyn_prob_list = [0, 0, 0, 0, 0]\n",
    "staten_prob_list = [0, 0, 0, 0, 0]\n",
    "man_prob_list = [0, 0, 0, 0, 0]\n",
    "queens_prob_list = [0, 0, 0, 0, 0]\n",
    "\n",
    "dic_all_boro = {}\n",
    "for b in bor_list:\n",
    "    dic_all_boro[b] = [0, 0, 0, 0, 0]\n",
    "\n",
    "temp_root = 0\n",
    "temp_trunk = 0\n",
    "temp_branch = 0\n",
    "temp_tot = 0\n",
    "sidewalk = 0\n",
    "for i in range (0, len(CB)):\n",
    "    if root_stone[i] == 'Yes':\n",
    "        temp_root += 1\n",
    "    if root_grate[i] == 'Yes':\n",
    "        temp_root += 1\n",
    "    if root_other[i] == 'Yes':\n",
    "        temp_root += 1\n",
    "    if trunk_wire[i] == 'Yes':\n",
    "        temp_trunk += 1\n",
    "    if trnk_light[i] == 'Yes':\n",
    "        temp_trunk += 1\n",
    "    if trnk_other[i] == 'Yes':\n",
    "        temp_trunk += 1\n",
    "    if brch_light[i] == 'Yes':\n",
    "        temp_branch += 1\n",
    "    if brch_shoe[i] == 'Yes':\n",
    "        temp_branch += 1\n",
    "    if brch_other[i] == 'Yes':\n",
    "        temp_branch += 1\n",
    "    if Sidewalk_Condition[i] == 'Damage':\n",
    "        sidewalk += 1\n",
    "    temp_tot = temp_root + temp_trunk + temp_branch + sidewalk\n",
    "    temp_list = [temp_root, temp_trunk, temp_branch, sidewalk, temp_tot]\n",
    "    \n",
    "    #choose which list to update\n",
    "    c = 0\n",
    "    for t in temp_list:\n",
    "        dic_all_boro[Borough[i]][c] += t\n",
    "        c += 1\n",
    "    temp_root = 0\n",
    "    temp_trunk = 0\n",
    "    temp_branch = 0\n",
    "    temp_tot = 0\n",
    "    sidewalk = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "\n",
    "root = [dic_all_boro['Bronx'][0], dic_all_boro['Brooklyn'][0] , dic_all_boro['Manhattan'][0] , dic_all_boro['Queens'][0] , dic_all_boro['Staten Island'][0]]\n",
    "trunk = [dic_all_boro['Bronx'][1], dic_all_boro['Brooklyn'][1], dic_all_boro['Manhattan'][1] , dic_all_boro['Queens'][1] , dic_all_boro['Staten Island'][1]]\n",
    "branch = [dic_all_boro['Bronx'][2], dic_all_boro['Brooklyn'][2], dic_all_boro['Manhattan'][2] , dic_all_boro['Queens'][2] , dic_all_boro['Staten Island'][2]]\n",
    "tot = [dic_all_boro['Bronx'][3], dic_all_boro['Brooklyn'][3], dic_all_boro['Manhattan'][3] , dic_all_boro['Queens'][3] , dic_all_boro['Staten Island'][3]]\n",
    "\n",
    "#f, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, sharex=True, sharey=True, figsize=(15,15))\n",
    "\n",
    "fig_index = 0\n",
    "#fig = plt.figure(fig_index)\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.set_xlabel('Root')\n",
    "ax.set_ylabel('Trunk')\n",
    "ax.scatter(np.asarray(root), np.asarray(trunk))\n",
    "for label, x, y in zip(bor_list, root, trunk):\n",
    "    ax.annotate(\n",
    "        label,\n",
    "        xy=(x, y), xytext=(-20, 20),\n",
    "        textcoords='offset points', ha='right', va='bottom',\n",
    "        bbox=dict(boxstyle='round,pad=0.5', fc='yellow', alpha=0.5),\n",
    "        arrowprops=dict(arrowstyle = '->', connectionstyle='arc3,rad=0'))\n",
    "fig.savefig(\"problem1\")\n",
    "plt.show()\n",
    "\n",
    "f, ax = plt.subplots()\n",
    "ax.set_xlabel('Root')\n",
    "ax.set_ylabel('Branch')\n",
    "ax.scatter(np.asarray(root), np.asarray(branch))\n",
    "for label, x, y in zip(bor_list, root, branch):\n",
    "    ax.annotate(\n",
    "        label,\n",
    "        xy=(x, y), xytext=(-20, 20),\n",
    "        textcoords='offset points', ha='right', va='bottom',\n",
    "        bbox=dict(boxstyle='round,pad=0.5', fc='yellow', alpha=0.5),\n",
    "        arrowprops=dict(arrowstyle = '->', connectionstyle='arc3,rad=0'))\n",
    "f.savefig(\"problem2\")\n",
    "plt.show()\n",
    "\n",
    "f, ax = plt.subplots()\n",
    "ax.set_xlabel('Trunk')\n",
    "ax.set_ylabel('Branch')\n",
    "ax.scatter(np.asarray(trunk), np.asarray(branch))\n",
    "for label, x, y in zip(bor_list, trunk, branch):\n",
    "    ax.annotate(\n",
    "        label,\n",
    "        xy=(x, y), xytext=(-20, 20),\n",
    "        textcoords='offset points', ha='right', va='bottom',\n",
    "        bbox=dict(boxstyle='round,pad=0.5', fc='yellow', alpha=0.5),\n",
    "        arrowprops=dict(arrowstyle = '->', connectionstyle='arc3,rad=0'))\n",
    "f.savefig(\"problem3\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have used the apriori algorithm for exploring if some problems happen to appear together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploration of problems:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "index = 0\n",
    "root_stone_lon = []\n",
    "root_stone_lat = []\n",
    "root_grate_lon = []\n",
    "root_grate_lat = []\n",
    "trunk_wire_lon = []\n",
    "trunk_wire_lat = []\n",
    "trunk_light_lon = []\n",
    "trunk_light_lat = []\n",
    "branch_light_lon = []\n",
    "branch_light_lat = []\n",
    "branch_shoe_lon = []\n",
    "branch_shoe_lat = []\n",
    "count_br = 0\n",
    "count2 = 0\n",
    "\n",
    "for i in range(0, len(Latitude)):\n",
    "    if root_stone[i] == 'Yes':\n",
    "        root_stone_lat.append(float(Latitude[i]))\n",
    "        root_stone_lon.append(float(Longitude[i]))\n",
    "    if root_grate[i] == 'Yes':\n",
    "        root_grate_lat.append(Latitude[i])\n",
    "        root_grate_lon.append(Longitude[i])\n",
    "    if trunk_wire[i] == 'Yes':\n",
    "        trunk_wire_lat.append(Latitude[i])\n",
    "        trunk_wire_lon.append(Longitude[i])\n",
    "    if trnk_light[i] == 'Yes':\n",
    "        trunk_light_lat.append(Latitude[i])\n",
    "        trunk_light_lon.append(Longitude[i])    \n",
    "    if brch_light[i] == 'Yes':\n",
    "        branch_light_lat.append(Latitude[i])\n",
    "        branch_light_lon.append(Longitude[i])    \n",
    "    if brch_shoe[i] == 'Yes':\n",
    "        branch_shoe_lat.append(Latitude[i])\n",
    "        branch_shoe_lon.append(Longitude[i])\n",
    "    if Borough[i] == 'Brooklyn' and brch_light[i] == 'Yes' and trunk_wire[i] == 'Yes':\n",
    "        count_br += 1\n",
    "    if Borough[i] == 'Brooklyn':\n",
    "        count2 += 1\n",
    "print 'Count: ', count_br, count2\n",
    "print (set(Borough))\n",
    "\n",
    "root_stone_zip = zip(root_stone_lon, root_stone_lat)\n",
    "root_grate_zip = zip(root_grate_lon, root_grate_lat)\n",
    "trunk_wire_zip = zip(trunk_wire_lon, trunk_wire_lat)\n",
    "trunk_light_zip = zip(trunk_light_lon, trunk_light_lat)\n",
    "branch_light_zip = zip(branch_light_lon,  branch_light_lat)\n",
    "branch_shoe_zip = zip(branch_shoe_lon, branch_shoe_lat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sidewalk_cond_lon = []\n",
    "sidewalk_cond_lat = []\n",
    "for i in range(0, len(Latitude)):\n",
    "    if Sidewalk_Condition[i] == 'Damage':\n",
    "        sidewalk_cond_lat.append(float(Latitude[i]))\n",
    "        sidewalk_cond_lon.append(float(Longitude[i]))\n",
    "with open('sidewalk_dam.csv', 'wb') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(('lon', 'lat'))\n",
    "    for xd, yd in zip(sidewalk_cond_lon, sidewalk_cond_lat):\n",
    "        writer.writerow( (xd, yd ) )\n",
    "    csvfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('root_stone.csv', 'wb') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(('lon', 'lat'))\n",
    "    for xd, yd in zip(root_stone_lon, root_stone_lat):\n",
    "        writer.writerow( (xd, yd ) )\n",
    "    csvfile.close()\n",
    "with open('root_grate.csv', 'wb') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(('lon', 'lat'))\n",
    "    for xd, yd in zip(root_grate_lon, root_grate_lat):\n",
    "        writer.writerow( (xd, yd ) )\n",
    "    csvfile.close()\n",
    "with open('trk_wire.csv', 'wb') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(('lon', 'lat'))\n",
    "    for xd, yd in  zip(trunk_wire_lon, trunk_wire_lat):\n",
    "        writer.writerow( (xd, yd ) )\n",
    "    csvfile.close()\n",
    "with open('trk_light.csv', 'wb') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(('lon', 'lat'))\n",
    "    for xd, yd in  zip(trunk_light_lon, trunk_light_lat):\n",
    "        writer.writerow( (xd, yd ) )\n",
    "    csvfile.close()\n",
    "with open('brc_light.csv', 'wb') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(('lon', 'lat'))\n",
    "    for xd, yd in  zip(branch_light_lon, branch_light_lat):\n",
    "        writer.writerow( (xd, yd ) )\n",
    "    csvfile.close()\n",
    "with open('brc_shoe.csv', 'wb') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(('lon', 'lat'))\n",
    "    for xd, yd in  zip(branch_shoe_lon, branch_shoe_lat):\n",
    "        writer.writerow( (xd, yd ) )\n",
    "    csvfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import geoplotlib\n",
    "from geoplotlib.utils import read_csv, BoundingBox, DataAccessObject\n",
    "\n",
    "min_lat = min(root_stone_lat)\n",
    "max_lat = max(root_stone_lat)\n",
    "min_lon = min(root_stone_lon)\n",
    "max_lon = max(root_stone_lon)\n",
    "\n",
    "bbox = BoundingBox(north=float(max_lat), west=float(max_lon), south=float(min_lat), east=float(min_lon))\n",
    "print \"Trees:\", bbox\n",
    "\n",
    "\n",
    "geoplotlib.set_bbox(bbox)\n",
    "\n",
    "data = read_csv('root_stone.csv')\n",
    "print 'Root stone: '\n",
    "geoplotlib.dot(data, 'r', point_size = 0.4)\n",
    "geoplotlib.inline()\n",
    "\n",
    "data = read_csv('root_grate.csv')\n",
    "print 'Root grate: '\n",
    "geoplotlib.dot(data, 'r', point_size = 0.4)\n",
    "geoplotlib.inline()\n",
    "\n",
    "data = read_csv('trk_wire.csv')\n",
    "print 'Trunk wire: '\n",
    "geoplotlib.dot(data, 'r', point_size = 0.4)\n",
    "geoplotlib.inline()\n",
    "\n",
    "data = read_csv('trk_light.csv')\n",
    "print 'Trunk light: '\n",
    "geoplotlib.dot(data, 'r', point_size = 0.4)\n",
    "geoplotlib.inline()\n",
    "\n",
    "data = read_csv('brc_light.csv')\n",
    "print 'Branch light: '\n",
    "geoplotlib.dot(data, 'r', point_size = 0.4)\n",
    "geoplotlib.inline()\n",
    "\n",
    "data = read_csv('brc_shoe.csv')\n",
    "print 'Branch shoe: '\n",
    "geoplotlib.dot(data, 'r', point_size = 0.4)\n",
    "geoplotlib.inline()\n",
    "\n",
    "data1 = read_csv('brc_light.csv')\n",
    "data2 = read_csv('trk_wire.csv')\n",
    "print 'Branch light and Trunk Wire: '\n",
    "geoplotlib.dot(data1, 'r', point_size = 0.4)\n",
    "geoplotlib.dot(data2, 'g', point_size = 0.4)\n",
    "geoplotlib.inline()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = read_csv('sidewalk_dam.csv')\n",
    "print 'Sidewalk damaged: '\n",
    "geoplotlib.dot(data, 'r', point_size = 0.4)\n",
    "geoplotlib.inline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!pip install apyori-1.1.1.tar.gz\n",
    "## Trying association mining\n",
    "from apyori import apriori\n",
    "\n",
    "transactions = [\n",
    "    ['cheese', 'nuggets'],\n",
    "    ['burgers', 'balls'],\n",
    "]\n",
    "results = list(apriori(transactions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Trying association mining\n",
    "from apyori import apriori\n",
    "\n",
    "transactions = [\n",
    "    ['beer', 'nuts'],\n",
    "    ['beer', 'cheese'],\n",
    "    ['nuts', 'cheese'],\n",
    "]\n",
    "transactions.append(['nuts', 'cheese'])\n",
    "results = list(apriori(transactions))\n",
    "print results[0]\n",
    "print ''\n",
    "print results[1]\n",
    "print ''\n",
    "print results[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#root_stone = data['root_stone']\n",
    "#root_grate = data['root_grate']\n",
    "#root_other = data['root_other']\n",
    "#trunk_wire = data['trunk_wire']\n",
    "#trnk_light = data['trnk_light']\n",
    "#trnk_other = data['trnk_other']\n",
    "#brch_light = data['brch_light']\n",
    "#brch_shoe = data['brch_shoe']\n",
    "#brch_other = data['brch_other']\n",
    "transactions = []\n",
    "temp = []\n",
    "np_count = 0\n",
    "nuno_count = 0\n",
    "counter = 0\n",
    "print len(temp)\n",
    "for i in range(0,len(root_stone)):\n",
    "    temp = []\n",
    "    if root_stone[i] == 'Yes':\n",
    "        temp.append(\"Root_Stone\")\n",
    "    if root_grate[i] == 'Yes':\n",
    "        temp.append(\"Root_Grate\")\n",
    "    #if root_other[i] == 'Yes':\n",
    "        #temp.append(\"Root_Other\")\n",
    "    if trunk_wire[i] == 'Yes':\n",
    "        temp.append(\"Trunk_Wire\")\n",
    "    if trnk_light[i] == 'Yes':\n",
    "        temp.append(\"Trunk_Light\")\n",
    "    #if trnk_other[i] == 'Yes':\n",
    "        #temp.append(\"Trunk_Other\")\n",
    "    if brch_light[i] == 'Yes':\n",
    "        temp.append(\"Branch_Light\")\n",
    "    if brch_shoe[i] == 'Yes':\n",
    "        temp.append(\"Branch_Shoe\")\n",
    "    if Sidewalk_Condition[i] == 'Damage':\n",
    "        temp.append(\"Sidewalk\")\n",
    "    #if brch_other[i] == 'Yes':\n",
    "        #temp.append(\"Branch_Other\")\n",
    "    if (len(temp)) > 1:    \n",
    "        transactions.append(temp)\n",
    "    elif (len(temp)) == 0: \n",
    "        np_count = np_count + 1\n",
    "    elif (len(temp)) == 1: \n",
    "        nuno_count += 1\n",
    "    if (len(temp)) > 1:\n",
    "        counter += 1\n",
    "        \n",
    "results = list(apriori(np.asarray(transactions)))\n",
    "print 'Associated:', len(transactions)\n",
    "print len(results)\n",
    "print 'Empty:', np_count\n",
    "print 'One Item:', nuno_count\n",
    "print 'More: ', counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "transactions = []\n",
    "temp = []\n",
    "np_count = 0\n",
    "nuno_count = 0\n",
    "counter = 0\n",
    "print len(temp)\n",
    "for i in range(0,len(root_stone)):\n",
    "    temp = []\n",
    "    if root_stone[i] == 'Yes':\n",
    "        temp.append(\"Root_Stone\")\n",
    "    #if root_grate[i] == 'Yes':\n",
    "    #    temp.append(\"Root_Grate\")\n",
    "    if trunk_wire[i] == 'Yes':\n",
    "        temp.append(\"Trunk_Wire\")\n",
    "    #if trnk_light[i] == 'Yes':\n",
    "    #    temp.append(\"Trunk_Light\")\n",
    "    if brch_light[i] == 'Yes':\n",
    "        temp.append(\"Branch_Light\")\n",
    "    #if brch_shoe[i] == 'Yes':\n",
    "    #    temp.append(\"Branch_Shoe\")\n",
    "    #if Sidewalk_Condition[i] == 'Damage':\n",
    "    #    temp.append(\"Sidewalk\")\n",
    "    if (len(temp)) > 1:    \n",
    "        transactions.append(temp)\n",
    "    elif (len(temp)) == 0: \n",
    "        np_count = np_count + 1\n",
    "    elif (len(temp)) == 1: \n",
    "        nuno_count += 1\n",
    "    if (len(temp)) > 1:\n",
    "        counter += 1\n",
    "        \n",
    "results = list(apriori(np.asarray(transactions)))\n",
    "for i in range (0, len(results)):\n",
    "    print '- ', i, ':', results[i][0], results[i][1], ', Lift:' ,results[i][-1][-1][-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results show us that branch light appears together with trunk wire, which can also be seen in the plot. This could be because trees places such that their branches grow into street lights also have the trunk be provoked by the lighting structures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have also explored the 311 dataset in the context of trees, as there is some data in there that is specific to our domain. \n",
    "\n",
    "Some of the most interesting 311 requests that we found, were in relation to overgrown trees and new tree requests. The Python analysis has not been included in detail in the notebook, but two images are included which show two geoplots of the mentioned complaints (note that there are some hotspots )."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(\"new_requests.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Image(\"overgrown_trees.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have found out that, sadly, problems are not related with the health. In fact, the Pearson correlation of these two parameters was very low."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We explore the diameter and the amount of trees in area of the city for discovering that these two factors are influencing the air quality. We also discovered that the problems seem to have an influence on the diameter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Image(\"problem_amount.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above image shows the resuls of the regression, which can also be seen here:\n",
    "\n",
    "* 0.28485178"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression\n",
    "These are the results of the regression between air pollution and O2 for predicting the amount of air pollution (and O2) given the amount of trees and their diameter as parameters. The results were almost the same for the other types of particles found. The whole regression notebook is included in the repo with the necessary datafiles. (https://github.com/cecli/cecli.github.io/blob/master/regression_notebook.ipynb)\n",
    "\n",
    "* r^2 elastic net on test data : 0.509594\n",
    "* Mean squared error: 5.42\n",
    "* Mean squared error: 5.36\n",
    "* r^2 lasso on test data : 0.515746\n",
    "* Mean squared error: 5.52\n",
    "* Variance score (ols): 0.50\n",
    "\n",
    "The images showing the corellation between the amount of trees and the pollution is in the other notebook, and this is also the data which the regression visualizaiton on the website is based on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting species based on location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#KNN classifier\n",
    "\n",
    "#Load relevant libraries\n",
    "import numpy as np\n",
    "import pylab as pl\n",
    "from sklearn import neighbors, datasets, model_selection\n",
    "\n",
    "#Split data set into a training and a test set\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(zip(tree_data['Latitude'], tree_data['Longitude'] )\n",
    "                                                    , tree_data['Spc_Common']\n",
    "                                                    , test_size=0.15\n",
    "                                                    , random_state=42)\n",
    "\n",
    "\n",
    "accuracy = []\n",
    "#Classify KNN with K=2-10\n",
    "for k in range(2,11):\n",
    "    knn = neighbors.KNeighborsClassifier(n_neighbors=k, weights = \"distance\")\n",
    "\n",
    "    #Fit the data and make predictions\n",
    "    knn.fit(X_train, y_train).predict(X_test)\n",
    "\n",
    "    #Calculate accuracy from validation set\n",
    "    n_folds = 5\n",
    "    score = np.mean(model_selection.cross_val_score(knn.fit(X_train, y_train),X_train, y_train,cv=n_folds))\n",
    "    print \"KNN score for k =\", k, \":\", score\n",
    "    \n",
    "    #Save accuracy into a list\n",
    "    accuracy.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Plot accuracy as a function of the number of K (2-10)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(20,5))\n",
    "ks = range(2, 11)\n",
    "plt.plot(ks, accuracy)\n",
    "plt.xticks(ks)\n",
    "plt.xlabel(\"k\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Prediction accuracy as a function of k\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#K=4 was chosen for simplicity compared to accuracy\n",
    "\n",
    "#Test score\n",
    "knn = neighbors.KNeighborsClassifier(n_neighbors=4, weights = \"distance\")\n",
    "\n",
    "#Fit the data and make predictions\n",
    "knn.fit(X_train, y_train).predict(X_test)\n",
    "\n",
    "#Calculate accuracy\n",
    "score = knn.fit(X_train, y_train).score(X_test, y_test)\n",
    "print score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tree_data = pd.read_csv('2015_tree_data_updated.csv')\n",
    "\n",
    "unique, counts = np.unique(zip(tree_data['Spc_Common']), return_counts=True)\n",
    "print sorted(zip(counts, unique), reverse = True)\n",
    "#Try doing KNN for only the top 5 species\n",
    "top5_spec = ['London planetree','honeylocust', 'Callery pear','pin oak', 'Norway maple']\n",
    "tree_spec5 = []\n",
    "tree_lat5 = []\n",
    "tree_lon5 = []\n",
    "for i in range(len(tree_data)):\n",
    "    if tree_data['Spc_Common'][i] in top5_spec:\n",
    "        tree_spec5.append(tree_data['Spc_Common'][i])\n",
    "        tree_lat5.append(tree_data['Latitude'][i])\n",
    "        tree_lon5.append(tree_data['Longitude'][i])\n",
    "print len(tree_spec5)\n",
    "#print tree_spec5[:10]\n",
    "#print tree_lat5[:10]\n",
    "#print tree_lon5[:10]\n",
    "\n",
    "#KNN classifier\n",
    "#Load relevant libraries\n",
    "import numpy as np\n",
    "import pylab as pl\n",
    "from sklearn import neighbors, datasets, model_selection\n",
    "\n",
    "#Split data set into a training and a test set\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(zip(tree_lat5, tree_lon5)\n",
    "                                                    , tree_spec5\n",
    "                                                    , test_size=0.15\n",
    "                                                    , random_state=42)\n",
    "\n",
    "accuracy = []\n",
    "#Classify KNN with K=2-10\n",
    "for k in range(2,11):\n",
    "    knn = neighbors.KNeighborsClassifier(n_neighbors=k, weights = \"distance\")\n",
    "    \n",
    "    #Fit the data and make predictions\n",
    "    knn.fit(X_train, y_train).predict(X_test)\n",
    "\n",
    "    #Calculate accuracy\n",
    "    #score = knn.fit(X_train, y_train).score(X_test, y_test)\n",
    "    n_folds = 5\n",
    "    score = np.mean(model_selection.cross_val_score(knn.fit(X_train, y_train),X_train, y_train,cv=n_folds))\n",
    "    print \"KNN score for k =\", k, \":\", score\n",
    "    \n",
    "    #Save accuracy into a list\n",
    "    accuracy.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Create Decision tree classifier\n",
    "\n",
    "#Load relevant libraries\n",
    "import numpy as np\n",
    "from sklearn import tree\n",
    "from sklearn import model_selection\n",
    "\n",
    "#Split data set into a training and a test set\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(zip(tree_data['Latitude'], tree_data['Longitude'])\n",
    "                                                    , tree_data['Spc_Common']\n",
    "                                                    , test_size=0.15\n",
    "                                                    , random_state=42)\n",
    "\n",
    "#Classify Decision trees\n",
    "dt = tree.DecisionTreeClassifier(random_state = 42)\n",
    "\n",
    "#Fit the data and make predictions\n",
    "dt.fit(X_train, y_train).predict(X_test)\n",
    "\n",
    "#Calculate accuracy\n",
    "#score = dtnn.fit(X_train, y_train).score(X_test, y_test)\n",
    "n_folds = 5\n",
    "score = np.mean(model_selection.cross_val_score(dt.fit(X_train, y_train),X_train, y_train,cv=n_folds))\n",
    "print \"Decision tree accuracy:\", score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify health based on location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Adjust KNN classifyer\n",
    "\n",
    "#Load relevant libraries\n",
    "import numpy as np\n",
    "import pylab as pl\n",
    "from sklearn import neighbors, datasets, model_selection\n",
    "\n",
    "#Split data set into a training and a test set\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(zip(tree_data['Latitude'], tree_data['Longitude'])\n",
    "                                                    , tree_data['Health']\n",
    "                                                    , test_size=0.15\n",
    "                                                    , random_state=42)\n",
    "\n",
    "accuracy = []\n",
    "#Classify KNN with K=2-10\n",
    "for k in range(2,11):\n",
    "    knn = neighbors.KNeighborsClassifier(n_neighbors=k, weights=\"distance\")\n",
    "\n",
    "    #Fit the data and make predictions\n",
    "    knn_pred = knn.fit(X_train, y_train).predict(X_test)\n",
    "\n",
    "    #Calculate accuracy\n",
    "    #score = knn.fit(X_train, y_train).score(X_test, y_test)\n",
    "    n_folds = 5\n",
    "    score = np.mean(model_selection.cross_val_score(knn.fit(X_train, y_train),X_train, y_train,cv=n_folds))\n",
    "    print \"KNN score for k =\", k, \":\", score\n",
    "    \n",
    "    #Save accuracy into a list\n",
    "    accuracy.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Test accuracy\n",
    "\n",
    "knn = neighbors.KNeighborsClassifier(n_neighbors=5, weights=\"distance\")\n",
    "#Fit the data and make predictions\n",
    "knn_pred = knn.fit(X_train, y_train).predict(X_test)\n",
    "\n",
    "#Calculate accuracy\n",
    "score = knn.fit(X_train, y_train).score(X_test, y_test)\n",
    "print score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Create Decision tree classifier\n",
    "\n",
    "#Load relevant libraries\n",
    "import numpy as np\n",
    "from sklearn import tree\n",
    "from sklearn import model_selection\n",
    "\n",
    "\n",
    "#Split data set into a training and a test set\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(zip(tree_data['Latitude'], tree_data['Longitude'])\n",
    "                                                    , tree_data['Health']\n",
    "                                                    , test_size=0.15\n",
    "                                                    , random_state=42)\n",
    "\n",
    "#Classify Decision trees\n",
    "dt = tree.DecisionTreeClassifier(random_state = 42)\n",
    "\n",
    "#Fit the data and make predictions\n",
    "dt_pred = dt.fit(X_train, y_train).predict(X_test)\n",
    "\n",
    "#Calculate accuracy\n",
    "#score = dtnn.fit(X_train, y_train).score(X_test, y_test)\n",
    "n_folds = 5\n",
    "score = np.mean(model_selection.cross_val_score(dt.fit(X_train, y_train),X_train, y_train,cv=n_folds))\n",
    "print \"Decision tree accuracy:\", score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create SVM classifier\n",
    "\n",
    "#Load relevant libraries\n",
    "import numpy as np\n",
    "from sklearn import svm\n",
    "from sklearn import model_selection\n",
    "\n",
    "#Split data set into a training and a test set\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(zip(tree_data['Latitude'], tree_data['Longitude'])\n",
    "                                                    , tree_data['Health']\n",
    "                                                    , test_size=0.15\n",
    "                                                    , random_state=42)\n",
    "\n",
    "#Classify Decision trees\n",
    "svm = svm.SVC(random_state = 42)\n",
    "\n",
    "#Fit the data and make predictions\n",
    "svm.fit(X_train, y_train).predict(X_test)\n",
    "\n",
    "#Calculate accuracy\n",
    "#score = dtnn.fit(X_train, y_train).score(X_test, y_test)\n",
    "n_folds = 5\n",
    "score = np.mean(model_selection.cross_val_score(svm.fit(X_train, y_train),X_train, y_train,cv=n_folds))\n",
    "print \"SVM accuracy:\", score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision tree and Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We tried decision tree and random forest to predict species based on locations and diameter. We tried to classify both top 20, 10 and 5 species. We found out that the decision tree  was not an idea solution, as the best result was 0.5 for the top 5 species. Random forest had the same result. We have included the images of the decision trees, and as can be seen, they just predict two species: Locust and Pine Oak. So this is clearly not a good model. We also tried predicting neighbourhoods based on problems, but that also did not work, as the accuracy was even worse. The images are included in the repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Try doing KNN for only the top 10 species\n",
    "top10_spec = ['London planetree','pin oak', 'honeylocust','Norway maple', 'Callery pear']\n",
    "tree_spec10 = []\n",
    "tree_lat10 = []\n",
    "tree_lon10 = []\n",
    "tree_health10 = []\n",
    "tree_nth10 = []\n",
    "tree_diam10 = []\n",
    "tree_cb10 = []\n",
    "tree_boro10 = []\n",
    "tree_root10 = []\n",
    "tree_branch10 = []\n",
    "tree_trunk10 = []\n",
    "tree_total10 = []\n",
    "print len(health), len(tree_data)\n",
    "for i in range(len(health)):\n",
    "    if tree_data['Spc_Common'][i] in top10_spec and float(tree_data['Diameter'][i]) >= 10.00:\n",
    "        tree_spec10.append(tree_data['Spc_Common'][i])\n",
    "        tree_lat10.append(tree_data['Latitude'][i])\n",
    "        tree_lon10.append(tree_data['Longitude'][i])\n",
    "        tree_health10.append(health[i])\n",
    "        tree_nth10.append(tree_data['Neighbourhoods'][i])\n",
    "        tree_diam10.append(tree_data['Diameter'][i])\n",
    "        tree_cb10.append(tree_data['CB'][i])\n",
    "        tree_boro10.append(tree_data['Borough'][i])\n",
    "        tree_root10.append(root_list[i])\n",
    "        tree_branch10.append(branch_list[i])\n",
    "        tree_trunk10.append(trunk_list[i])\n",
    "        tree_total10.append(total_prob_list[i])\n",
    "print len(tree_spec10)\n",
    "print tree_spec10[:10]\n",
    "print tree_lat10[:10]\n",
    "print tree_lon10[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bin1 = 1.00\n",
    "bin2 = 10.00\n",
    "bin3 = 15.00\n",
    "bin4 = 20.00\n",
    "bin5 = 25.00\n",
    "bin6 = 30.00\n",
    "bin7 = 35.00\n",
    "bin8 = 40.00\n",
    "bin9 = 45.00\n",
    "bin10 = 50.00\n",
    "bin11 = 60.00\n",
    "bin12 = 70.00\n",
    "bin13 = 80.00\n",
    "bin14 = 90.00\n",
    "bin15 = 100.00\n",
    "bin16 = 150.00\n",
    "bin17 = 200.00\n",
    "bin18 = 250.00\n",
    "bin19 = 300.00\n",
    "list_bins = [bin19, bin18, bin17, bin16, bin15, bin14, bin13, bin12, bin11, bin10, bin9, bin8, \n",
    "             bin7, bin6, bin5, bin4, bin3, bin2]\n",
    "new_diam = []\n",
    "for t in tree_diam10: \n",
    "    ft = float(t)\n",
    "    for l in list_bins: \n",
    "        if ft >= l: \n",
    "            new_diam.append(l)\n",
    "            break\n",
    "        else: \n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Decision tree for classifying tree species based on health and diameter \n",
    "#Load relevant libraries\n",
    "import numpy as np\n",
    "from sklearn import tree\n",
    "from sklearn import model_selection\n",
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "#le.fit(tree_data['Neighbourhoods'])\n",
    "#list(le.classes_)\n",
    "#trans_nbh = le.transform(tree_data['Neighbourhoods']) \n",
    "\n",
    "#Split data set into a training and a test set\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(zip(tree_data['Diameter'], tree_data['Longitude'], tree_data['Latitude'])\n",
    "                                                    , tree_data['Spc_Common']\n",
    "                                                    , test_size=0.10\n",
    "                                                    , random_state=42)\n",
    "\n",
    "#Classify Decision trees\n",
    "dt = tree.DecisionTreeClassifier(random_state = 42)\n",
    "\n",
    "#Fit the data and make predictions\n",
    "dt.fit(X_train, y_train).predict(X_test)\n",
    "\n",
    "#Calculate accuracy\n",
    "#score = dtnn.fit(X_train, y_train).score(X_test, y_test)\n",
    "n_folds = 10\n",
    "score = np.mean(model_selection.cross_val_score(dt.fit(X_train, y_train),X_train, y_train,cv=n_folds))\n",
    "print \"Decision tree accuracy:\", score\n",
    "\n",
    "#trans_nbh10 = le.transform(tree_nth10) \n",
    "\n",
    "#X_train, X_test, y_train, y_test = model_selection.train_test_split(zip(tree_diam10, tree_health10, trans_cb10)\n",
    "                                                    #, tree_spec10\n",
    "                                                    #, test_size=0.33\n",
    "                                                    #, random_state=42)\n",
    "#Classify Decision trees\n",
    "#dt = tree.DecisionTreeClassifier(random_state = 42)\n",
    "\n",
    "#Fit the data and make predictions\n",
    "#dt.fit(X_train, y_train).predict(X_test)\n",
    "\n",
    "#Calculate accuracy\n",
    "#score = dtnn.fit(X_train, y_train).score(X_test, y_test)\n",
    "#n_folds = 10\n",
    "#score = np.mean(model_selection.cross_val_score(dt.fit(X_train, y_train),X_train, y_train,cv=n_folds))\n",
    "#print \"Decision tree accuracy for classifying top 10 species:\", score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import tree\n",
    "from sklearn import model_selection\n",
    "from sklearn import preprocessing\n",
    "from sklearn import ensemble\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(tree_nth10)\n",
    "list(le.classes_)\n",
    "trans_nbh10 = le.transform(tree_nth10) \n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(tree_boro10)\n",
    "list(le.classes_)\n",
    "trans_boro10 = le.transform(tree_boro10) \n",
    "\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(zip(tree_lon10, tree_lat10),\n",
    "                                                     tree_spec10\n",
    "                                                    , test_size=0.10\n",
    "                                                    , random_state=42)\n",
    "#Classify Decision trees\n",
    "dt = tree.DecisionTreeClassifier(random_state = 42)\n",
    "dt2 = ensemble.RandomForestClassifier(random_state = 42)\n",
    "\n",
    "#Fit the data and make predictions\n",
    "dt.fit(X_train, y_train).predict(X_test)\n",
    "dt2.fit(X_train, y_train).predict(X_test)\n",
    "\n",
    "#Calculate accuracy\n",
    "score = dtnn.fit(X_train, y_train).score(X_test, y_test)\n",
    "n_folds = 10\n",
    "score = np.mean(model_selection.cross_val_score(dt.fit(X_train, y_train),X_train, y_train,cv=n_folds))\n",
    "print \"Decision tree accuracy for classifying top 10 species:\", score\n",
    "score = np.mean(model_selection.cross_val_score(dt2.fit(X_train, y_train),X_train, y_train,cv=n_folds))\n",
    "print \"Random forest accuracy for classifying top 10 species:\", score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import tree\n",
    "from sklearn import model_selection\n",
    "from sklearn import preprocessing\n",
    "from sklearn import ensemble\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(tree_nth10)\n",
    "list(le.classes_)\n",
    "trans_nbh10 = le.transform(tree_nth10) \n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(tree_boro10)\n",
    "list(le.classes_)\n",
    "trans_boro10 = le.transform(tree_boro10) \n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(tree_spec10)\n",
    "list(le.classes_)\n",
    "trans_species10 = le.transform(tree_spec10) \n",
    "\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(zip(trans_species10, tree_total10)\n",
    "                                                    , new_diam\n",
    "                                                    , test_size=0.25\n",
    "                                                    , random_state=42)\n",
    "#Classify Decision trees\n",
    "dt = tree.DecisionTreeClassifier(max_depth=20, max_leaf_nodes=40, random_state = 42)\n",
    "#dt2 = ensemble.RandomForestClassifier(random_state = 42)\n",
    "\n",
    "#Fit the data and make predictions\n",
    "dt.fit(X_train, y_train).predict(X_test)\n",
    "#dt2.fit(X_train, y_train).predict(X_test)\n",
    "\n",
    "#Calculate accuracy\n",
    "#score = dtnn.fit(X_train, y_train).score(X_test, y_test)\n",
    "n_folds = 5\n",
    "score = np.mean(model_selection.cross_val_score(dt.fit(X_train, y_train),X_train, y_train,cv=n_folds))\n",
    "print \"Decision tree accuracy for classifying top 10 species:\", score\n",
    "#score = np.mean(model_selection.cross_val_score(dt2.fit(X_train, y_train),X_train, y_train,cv=n_folds))\n",
    "#print \"Random forest accuracy for classifying top 10 species:\", score"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
